import litellm
import dspy
from dspy.teleprompt import MIPROv2

litellm.drop_params = True

# -----------------------------------------------------------------
# 1. CONFIGURE DSPy FOR LOCAL OLLAMA
# -----------------------------------------------------------------
# Use the 'ollama_chat/' prefix to tell DSPy to route through LiteLLM to Ollama
local_model = dspy.LM(
    model='ollama_chat/llama3', # Make sure this matches the model you pulled
    api_base='http://localhost:11434', 
    api_key='ollama' # Placeholder, required by the underlying library but not used by Ollama
)

# Set it as the global default for all DSPy operations
dspy.configure(lm=local_model)

# -----------------------------------------------------------------
# 2. PREPARE THE DATASET
# -----------------------------------------------------------------
raw_data = [
    {"text": "The battery life on this phone is terrible.", "sentiment": "negative"},
    {"text": "I absolutely love the new camera features!", "sentiment": "positive"},
    {"text": "It arrived on time, but the packaging was damaged.", "sentiment": "mixed"},
    {"text": "Customer service was incredibly helpful.", "sentiment": "positive"}
]

trainset = [
    dspy.Example(text=d["text"], sentiment=d["sentiment"]).with_inputs("text")
    for d in raw_data
]


# -----------------------------------------------------------------
# 3. DEFINE SIGNATURE, MODULE, AND METRIC
# -----------------------------------------------------------------
class EmotionClassifier(dspy.Signature):
    """Classify the sentiment of the provided text."""
    text = dspy.InputField(desc="A review from a customer")
    sentiment = dspy.OutputField(desc="positive, negative, or mixed")

class AnalyzeSentiment(dspy.Module):
    def __init__(self):
        super().__init__()
        self.predictor = dspy.ChainOfThought(EmotionClassifier)
        
    def forward(self, text):
        return self.predictor(text=text)

def exact_match_metric(example, pred, trace=None):
    return example.sentiment.lower() == pred.sentiment.lower()

# -----------------------------------------------------------------
# 4. RUN COPRO OPTIMIZER LOCALLY
# -----------------------------------------------------------------
print("Starting COPRO Optimizer...")

mipro_optimizer = MIPROv2(
    metric=exact_match_metric,
    auto="light", # Options: "light" (~10 mins), "medium", or "heavy" (exhaustive search)
    verbose=True,
    num_threads=1
)

# Compile! This is where the Bayesian Optimization happens.
optimized_program = mipro_optimizer.compile(
    student=AnalyzeSentiment(),
    trainset=trainset,
    # max_bootstrapped_demos: How many examples the LLM verifies and formats itself
    max_bootstrapped_demos=3, 
    # max_labeled_demos: How many raw examples to pull directly from your dataset
    max_labeled_demos=2
)

# -----------------------------------------------------------------
# 5. TEST & INSPECT
# -----------------------------------------------------------------
test_text = "The screen resolution is okay, but not what I expected for the price."
prediction = optimized_program(text=test_text)
print(f"\nPrediction for test text: {prediction.sentiment}")

# View the actual optimized system prompt / instructions generated by your local model
print("\n--- Last Prompt Sent to Local Model ---")
local_model.inspect_history(n=1)