import litellm
import dspy
from dspy.teleprompt import COPRO

litellm.drop_params = True

# -----------------------------------------------------------------
# 1. CONFIGURE DSPy FOR LOCAL OLLAMA
# -----------------------------------------------------------------
# Use the 'ollama_chat/' prefix to tell DSPy to route through LiteLLM to Ollama
local_model = dspy.LM(
    model='ollama_chat/llama3', # Make sure this matches the model you pulled
    api_base='http://localhost:11434', 
    api_key='ollama' # Placeholder, required by the underlying library but not used by Ollama
)

# Set it as the global default for all DSPy operations
dspy.configure(lm=local_model)

# -----------------------------------------------------------------
# 2. PREPARE THE DATASET
# -----------------------------------------------------------------
raw_data = [
    {"text": "The battery life on this phone is terrible.", "sentiment": "negative"},
    {"text": "I absolutely love the new camera features!", "sentiment": "positive"},
    {"text": "It arrived on time, but the packaging was damaged.", "sentiment": "mixed"},
    {"text": "Customer service was incredibly helpful.", "sentiment": "positive"}
]

trainset = [
    dspy.Example(text=d["text"], sentiment=d["sentiment"]).with_inputs("text")
    for d in raw_data
]


# -----------------------------------------------------------------
# 3. DEFINE SIGNATURE, MODULE, AND METRIC
# -----------------------------------------------------------------
class EmotionClassifier(dspy.Signature):
    """Classify the sentiment of the provided text."""
    text = dspy.InputField(desc="A review from a customer")
    sentiment = dspy.OutputField(desc="positive, negative, or mixed")

class AnalyzeSentiment(dspy.Module):
    def __init__(self):
        super().__init__()
        self.predictor = dspy.Predict(EmotionClassifier)
        
    def forward(self, text):
        return self.predictor(text=text)

def exact_match_metric(example, pred, trace=None):
    return example.sentiment.lower() == pred.sentiment.lower()

# -----------------------------------------------------------------
# 4. RUN COPRO OPTIMIZER LOCALLY
# -----------------------------------------------------------------
print("Starting COPRO Optimizer...")

copro_optimizer = COPRO(
    metric=exact_match_metric,
    verbose=True
)

# Compile! 
compiled_program = copro_optimizer.compile(
    student=AnalyzeSentiment(),
    trainset=trainset,
    # IMPORTANT LOCAL SETTING: Set num_threads=1. 
    # Local Ollama servers usually can't handle multiple concurrent LLM streams 
    # without running out of memory or severely bottlenecking.
    eval_kwargs={"num_threads": 1, "display_progress": True} 
)

# -----------------------------------------------------------------
# 5. TEST & INSPECT
# -----------------------------------------------------------------
test_text = "The screen resolution is okay, but not what I expected for the price."
prediction = compiled_program(text=test_text)
print(f"\nPrediction for test text: {prediction.sentiment}")

# View the actual optimized system prompt / instructions generated by your local model
print("\n--- Last Prompt Sent to Local Model ---")
local_model.inspect_history(n=1)